# sygr0003 , UMU54907 , VT2019 , lab3_rl0 , simple RL (Reinforcment learning) demo 
# -*- coding: utf-8 -*-
"""Reinforcement Learning Pole Cart Gym.ipynb

Automatically generated by Colaboratory.

Original file is located at
  https://colab.research.google.com/drive/1hOTmbBSiZS1Ea77_7iwwpayvcjLNDDfZ
"""

# Inspired by RL researchers and written by Kalle Prorok for a Deep Learning Course
# Ume√• Sweden 2019
import random
import gym
import matplotlib.pyplot as plt

EPISODES = 1000
EPSILON = 0.1
GAMMA = 0.9
LEARNING_RATE = 0.1
DISCRETE_STEPS = 10   # 10 discretization steps per state variable


def argmax(l):
  """ Return the index of the maximum element of a list
  """
  return max(enumerate(l), key=lambda x:x[1])[0]


def make_state(observation):
  """ Map a 4-dimensional state to a state index
  """
  low = [-4.8, -10., -0.41, -10.]
  high = [4.8, 10., 0.41, 10.]
  state = 0

  for i in range(4):
    # State variable, projected to the [0, 1] range
    state_variable = (observation[i] - low[i]) / (high[i] - low[i])

    # Discretize. A variable having a value of 0.53 will lead to the integer 5,
    # for instance.
    state_discrete = int(state_variable * DISCRETE_STEPS)
    state_discrete = max(0, state_discrete)
    state_discrete = min(DISCRETE_STEPS-1, state_discrete)

    state *= DISCRETE_STEPS
    state += state_discrete

  return state

def main():
  average_cumulative_reward = 0.0

  # Create the Gym environment (CartPole)
  env = gym.make('CartPole-v1')
  

  print('Action space is:', env.action_space)
  print('Observation space is:', env.observation_space)

  #  plot  cummulative reward for some runs
  xval=[]
  yval=[]

  # Q-table for the discretized states, and two actions
  num_states = DISCRETE_STEPS ** 4
  qtable = [[0., 0.] for state in range(num_states)]

  # Loop over episodes
  for i in range(EPISODES):
    state = env.reset()
    state = make_state(state)

    terminate = False
    cumulative_reward = 0.0

    # Loop over time-steps
    # print("step,  cumulativ reward ")
    while not terminate:
      # Compute what the greedy action for the current state is
      qvalues = qtable[state]
      greedy_action = argmax(qvalues)

      # Sometimes, the agent takes a random action, to explore the environment
      if random.random() < EPSILON:
        action = random.randrange(2)
      else:
        action = greedy_action

      # Perform the action
      next_state, reward, terminate, info = env.step(action)  # info is ignored
      next_state = make_state(next_state)
      # Show the simulated environment. A bit difficult to make it work.      
      # env.render()
      #print(' Reward:',reward)
      # Update the Q-Table
      td_error = reward + GAMMA * max(qtable[next_state]) - qtable[state][action]
      qtable[state][action] += LEARNING_RATE * td_error
      # Update statistics
      cumulative_reward += reward
      state = next_state
    
    # Per-episode statistics
    if ((i % 100)==0):
      # print(i, cumulative_reward, sep=',')
      xval.append(i)
      yval.append(cumulative_reward)
      
  # plot results
  info= "epsilon:%.2f ,  lr: %.2f"   % (  EPSILON , LEARNING_RATE  ) 
  print (info)
  plt.figure(    "rl_prorok" , figsize=(5, 4))
  # ax = plt.subplot(2, 1, 1)
  
  plt.plot( xval , yval ,  'bo')
  plt.xlabel('episodes')
  plt.ylabel("cummulative rewards")
  theTitle =info 
  plt.title(theTitle)
  plt.legend()
  plt.savefig ("rlProrok")
  plt.show()
  


#if __name__ == '__main__':
main()
